{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrper\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pprint\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models import KeyedVectors\n",
    "from progress.bar import Bar\n",
    "\n",
    "\n",
    "class ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemN2N model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemN2N(object):\n",
    "    \n",
    "    def __init__(self,edim, nhop, mem_size, batch_size, nepoch, anneal_epoch, init_lr, anneal_rate,\n",
    "                 init_mean, init_std, max_grand_norm, data_dir, checkpoint_dir, lin_start, is_test, show_progress, sess):\n",
    "        self.nwords = 60 \n",
    "        self.max_words = 100\n",
    "        self.init_mean = init_mean\n",
    "        self.init_std = init_std\n",
    "        self.batch_size = batch_size\n",
    "        self.nepoch = nepoch\n",
    "        self.anneal_epoch = anneal_epoch\n",
    "        self.nhop = nhop\n",
    "        self.edim = edim\n",
    "        self.mem_size = mem_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.lin_start = lin_start\n",
    "        self.show_progress = show_progress\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        #nwords==> Total number of unique words in file ===> #V\n",
    "        \n",
    "        self.query = tf.compat.v1.placeholder(tf.int32, [None, self.max_words], name='input') #quesitons\n",
    "        #query ==>Tensor(\"input:0\", shape=(None, 100), dtype=int32)  ==> Question is 100 length vector so, query is 100 length vector\n",
    "        self.time = tf.compat.v1.placeholder(tf.int32, [None, self.mem_size], name='time')\n",
    "        #time ==> Tensor(\"time:0\", shape=(None, 50), dtype=int32)  \n",
    "        self.target = tf.compat.v1.placeholder(tf.float32, [None, self.nwords], name='target') #answers\n",
    "        #target==> Tensor(\"target:0\", shape=(None, 60), dtype=float32)\n",
    "        self.context = tf.compat.v1.placeholder(tf.int32, [None, self.mem_size, self.max_words], name='context')\n",
    "        #context ==> Tensor(\"context:0\", shape=(None, 50, 100), dtype=int32)\n",
    "        \n",
    "        self.hid = []\n",
    "        \n",
    "        self.lr = None\n",
    "        \n",
    "        #learning rate\n",
    "        if self.lin_start:\n",
    "            self.current_lr = 0.005\n",
    "        else:\n",
    "            self.current_lr = init_lr\n",
    "\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.loss = None\n",
    "        self.optim = None\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "    \n",
    "   \n",
    "    def build_memory(self):\n",
    "        print(\"build memory....\")\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "        \n",
    "        zeros = tf.constant(0, tf.float32, [1, self.edim])\n",
    "        #zeros==> Tensor(\"Const:0\", shape=(1, 20), dtype=float32)\n",
    "        self.A_ = tf.Variable(tf.compat.v1.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        #A_==> <tf.Variable 'Variable:0' shape=(59, 20) dtype=float32>\n",
    "        self.B_ = tf.Variable(tf.compat.v1.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        #B_==> <tf.Variable 'Variable_1:0' shape=(59, 20) dtype=float32> \n",
    "        self.C_ = tf.Variable(tf.compat.v1.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        #C_==> <tf.Variable 'Variable_2:0' shape=(59, 20) dtype=float32>\n",
    "        \n",
    "        A = tf.concat([zeros, self.A_], axis=0) #For predicates\n",
    "        #A==> Tensor(\"concat:0\", shape=(60, 20), dtype=float32) \n",
    "        B = tf.concat([zeros, self.B_], axis=0) # For question\n",
    "        #B=> Tensor(\"concat_1:0\", shape=(60, 20), dtype=float32)\n",
    "        C = tf.concat([zeros, self.C_], axis=0) #For predicates\n",
    "        #C==> Tensor(\"concat_2:0\", shape=(60, 20), dtype=float32)\n",
    "        \n",
    "        self.T_A_ = tf.Variable(tf.compat.v1.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.T_C_ = tf.Variable(tf.compat.v1.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        T_A = tf.concat([zeros, self.T_A_], axis=0)\n",
    "        #T_A==> Tensor(\"concat_3:0\", shape=(50, 20), dtype=float32)\n",
    "        T_C = tf.concat([zeros, self.T_C_], axis=0)\n",
    "        #T_C==> Tensor(\"concat_4:0\", shape=(50, 20), dtype=float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Embeddings for context(predicate) = A_ebd\n",
    "        A_ebd = tf.nn.embedding_lookup(A, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        A_ebd = tf.reduce_sum(A_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_A_ebd = tf.nn.embedding_lookup(T_A, self.time)  # [batch_size, mem_size, edim]\n",
    "        A_in = tf.add(A_ebd, T_A_ebd)                     # [batch_size, mem_size, edim]\n",
    "        #A_in==> Tensor(\"Add:0\", shape=(None, 50, 20), dtype=float32)\n",
    "        \n",
    "        #Embeddings for context (predicate) = C_ebd\n",
    "        C_ebd = tf.nn.embedding_lookup(C, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        C_ebd = tf.reduce_sum(C_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_C_ebd = tf.nn.embedding_lookup(T_C, self.time)  # [batch_size, mem_size, edim]\n",
    "        C_in = tf.add(C_ebd, T_C_ebd)                     # [batch_size, mem_size, edim]\n",
    "        # C_in==> Tensor(\"Add_1:0\", shape=(None, 50, 20), dtype=float32)\n",
    "        \n",
    "        #Embeddings for query (question) = B_emd ==> query_ebd\n",
    "        query_ebd = tf.nn.embedding_lookup(B, self.query) # [batch_size, max_length, edim]\n",
    "        query_ebd = tf.reduce_sum(query_ebd, axis=1)      # [batch_size, edim]\n",
    "        # query_ebd====> Tensor(\"Sum_2:0\", shape=(None, 20), dtype=float32)\n",
    "        self.hid.append(query_ebd) #store question embedding in hope id to embedding to the next hope\n",
    "        \n",
    "        for h in range(self.nhop):\n",
    "            q3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim]) # [batch_size, edim] ==> [batch_size, 1, edim]\n",
    "            p3dim = tf.matmul(q3dim, A_in, transpose_b=True)     # [batch_size, 1, edim] X [batch_size, edim, mem_size]\n",
    "            p2dim = tf.reshape(p3dim, [-1, self.mem_size])       # [batch_size, mem_size]\n",
    "            \n",
    "            # If linear start, remove softmax layers\n",
    "            if self.lin_start:\n",
    "                p = p2dim\n",
    "            else:\n",
    "                p = tf.nn.softmax(p2dim)\n",
    "            \n",
    "            p3dim = tf.reshape(p, [-1, 1, self.mem_size]) # [batch_size, 1, mem_size]\n",
    "            o3dim = tf.matmul(p3dim, C_in)                # [batch_size, 1, mem_size] X [batch_size, mem_size, edim]\n",
    "            o2dim = tf.reshape(o3dim, [-1, self.edim])    # [batch_size, edim]\n",
    "            \n",
    "            a = tf.add(o2dim, self.hid[-1]) # [batch_size, edim]\n",
    "            self.hid.append(a)              # [input, a_1, a_2, ..., a_nhop]\n",
    "    \n",
    "    def build_model(self):\n",
    "        print(\"Build model...\")\n",
    "        \n",
    "        #In memory building we are creating all embedding matrixs A,B,C and O. And also create number of memory units as per hops.\n",
    "        self.build_memory()\n",
    "        \n",
    "        #Creating weight matrix\n",
    "        self.W = tf.Variable(tf.compat.v1.random_normal([self.edim, self.nwords], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        #Finding final prediction after multiplying weight metrix with last embedding B ==> here access it using 'hid'\n",
    "        a_hat = tf.matmul(self.hid[-1], self.W)\n",
    "        \n",
    "        self.hypothesis = tf.nn.softmax(a_hat)\n",
    "        \n",
    "        #finding loss from prediction (a_hat) and target\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=a_hat, labels=self.target)\n",
    "        \n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.compat.v1.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        params = [self.A_, self.B_, self.C_, self.T_A_, self.T_C_, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss, params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) for gv in grads_and_vars]\n",
    "        \n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "        \n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "    def train(self,final_predicate_dict,predicates, train_questions):\n",
    "        N = int(math.ceil(len(train_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(train_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "                \n",
    "            #Query is array of shape [iterations,maxword] ==> train questions\n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            #time is Zero array of shape[iterations,mem_size]\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            #target is zeros array of [iterations,nwords] ==>Answer\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                curr_q = train_questions[m]\n",
    "                q_text = curr_q['sentance']\n",
    "                answer = curr_q['predicate']\n",
    "                predicate_index = final_predicate_dict[tuple(answer)]\n",
    "                \n",
    "\n",
    "                \n",
    "                #Currr_s is a current story\n",
    "                curr_s = predicates\n",
    "                #curr_c is a require story sentances for current quesiton (from current story)\n",
    "                curr_c = curr_s[0]\n",
    "                \n",
    "                #mem_size: maximum number of sentences that can be encoded into memory [50]\n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    #Take last M(memory size) sentances from curr_c\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t) #bcz time is an array of shape [iterations,mem_size]\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                #Query is train questions\n",
    "                #batch size 32 so that in one batch it will store only 32 q_text in query \n",
    "                query[b, :] = q_text #one by one questions put to the query\n",
    "                #Target is answer\n",
    "                target[b, predicate_index] = 1 #on which id we got ans put 1 over there. I.e for first we got 5 (idx) so put 1 on 5th index\n",
    "                context[b, :, :] = curr_c\n",
    "                #means this question and answer given based on given context\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(train_questions)\n",
    "    \n",
    "    \n",
    "    def test(self,final_predicate_dict,predicates, test_questions, label='Test'):\n",
    "        N = int(math.ceil(len(test_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(test_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = test_questions[m]\n",
    "                q_text = curr_q['sentance']\n",
    "                answer = curr_q['predicate']\n",
    "                predicate_index = final_predicate_dict[tuple(answer)]\n",
    "                \n",
    "                curr_s = predicates\n",
    "                curr_c = curr_s[0]\n",
    "                \n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, predicate_index] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(test_questions)\n",
    "    \n",
    "    \n",
    "    def run(self, final_predicate_dict,predicates, train_questions, test_questions):\n",
    "        if not self.is_test: #Training\n",
    "\n",
    "            for idx in range(self.nepoch):\n",
    "                #calling train\n",
    "                train_loss = np.sum(self.train(final_predicate_dict,predicates, train_questions))\n",
    "                #calling test\n",
    "                test_loss = np.sum(self.test(final_predicate_dict,predicates, test_questions, label='Validation'))\n",
    "                \n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                \n",
    "                state = {\n",
    "                    'loss': train_loss,\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'validation_loss': test_loss\n",
    "                }\n",
    "                \n",
    "                print(state)\n",
    "                \n",
    "                \n",
    "                # learning rate annealing\n",
    "                if (not idx == 0) and (idx % self.anneal_epoch == 0):\n",
    "                    self.current_lr = self.current_lr * self.anneal_rate\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "            \n",
    "                # If validation loss stops decreasing, insert softmax layers\n",
    "                if idx == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.log_loss[idx][1] > self.log_loss[idx - 1][1]:\n",
    "                        self.lin_start = False\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step=self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "            valid_loss = np.sum(self.test(final_predicate_dict,predicates, train_questions, label='Validation'))\n",
    "            test_loss = np.sum(self.test(final_predicate_dict,predicates, test_questions, label='Test'))\n",
    "            \n",
    "            state = {\n",
    "                'validation_loss': valid_loss,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "            \n",
    "            print(state)\n",
    "\n",
    "\n",
    "    def predict(self,final_predicate_dict,predicates, test_questions):\n",
    "        self.load()\n",
    "\n",
    "        num_instances = len(test_questions)\n",
    "\n",
    "        query = np.ndarray([num_instances, self.max_words], dtype=np.int32)\n",
    "        time = np.zeros([num_instances, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([num_instances, self.nwords], dtype=np.float32)\n",
    "        context = np.ndarray([num_instances, self.mem_size, self.max_words], dtype=np.int32)\n",
    "\n",
    "        for b in range(num_instances):\n",
    "            \n",
    "            curr_q = test_questions[b]\n",
    "            q_text = curr_q['sentance']\n",
    "            answer = curr_q['predicate']\n",
    "            predicate_index = final_predicate_dict[tuple(answer)]\n",
    "            \n",
    "            curr_s = predicates \n",
    "            curr_c = curr_s[0]\n",
    "            \n",
    "            if len(curr_c) >= self.mem_size:\n",
    "                curr_c = curr_c[-self.mem_size:]\n",
    "                \n",
    "                for t in range(self.mem_size):\n",
    "                    time[b, t].fill(t)\n",
    "            else:\n",
    "                \n",
    "                for t in range(len(curr_c)):\n",
    "                    time[b, t].fill(t)\n",
    "                \n",
    "                while len(curr_c) < self.mem_size:\n",
    "                    curr_c.append([0.] * self.max_words)\n",
    "            \n",
    "            query[b, :] = q_text\n",
    "            target[b, predicate_index] = 1\n",
    "            context[b, :, :] = curr_c\n",
    "\n",
    "        predictions = self.sess.run(self.hypothesis, feed_dict={self.query: query, self.time: time, self.context: context})\n",
    "\n",
    "        return predictions, target\n",
    "\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        print(' [*] Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] No checkpoint found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default values:\n",
      "internal state dimension (edim): 20\n",
      "number of hops (nhop): 3\n",
      "maximum number of sentences that can be encoded into memory (mem_size) : 50\n",
      "batch size to use during training (batch_size): 32\n",
      "number of epoch to use during training (nepoch): 100\n",
      "anneal the learning rate every <anneal_epoch> epochs (annel_epoch): 25\n",
      "initial learning rate (init_lr): 0.01\n",
      "learning rate annealing rate (annel_rate): 0.5\n",
      "weight initialization mean (init_mean): 0.\n",
      "weight initialization std (init_std): 0.1\n",
      "clip gradients to this norm (max_grad_norm): 40\n",
      "Do you want to use default values?-- Y/N\n",
      "Enter Y / N :Y\n"
     ]
    }
   ],
   "source": [
    "print(\"Default values:\")\n",
    "print(\"internal state dimension (edim): 20\")\n",
    "print(\"number of hops (nhop): 3\")\n",
    "print(\"maximum number of sentences that can be encoded into memory (mem_size) : 50\")\n",
    "print(\"batch size to use during training (batch_size): 32\")\n",
    "print(\"number of epoch to use during training (nepoch): 100\")\n",
    "print(\"anneal the learning rate every <anneal_epoch> epochs (annel_epoch): 25\")\n",
    "print(\"initial learning rate (init_lr): 0.01\")\n",
    "print(\"learning rate annealing rate (annel_rate): 0.5\")\n",
    "print(\"weight initialization mean (init_mean): 0.\")\n",
    "print(\"weight initialization std (init_std): 0.1\")\n",
    "print(\"clip gradients to this norm (max_grad_norm): 40\")\n",
    "print(\"Do you want to use default values?-- Y/N\")\n",
    "value = input(\"Enter Y / N :\")\n",
    "if value == 'Y':\n",
    "    edim = 20\n",
    "    nhop = 3\n",
    "    mem_size = 50\n",
    "    batch_size = 32\n",
    "    nepoch = 100\n",
    "    anneal_epoch = 25\n",
    "    init_lr = 0.01\n",
    "    anneal_rate = 0.5\n",
    "    init_mean = 0.\n",
    "    init_std = 0.1\n",
    "    max_grad_norm = 40\n",
    "    data_dir = \"./contextEncoder/Encoded_Data\"\n",
    "    checkpoint_dir =  \"./checkpoints\"\n",
    "    lin_start = False\n",
    "    is_test = False\n",
    "    show_progress = False\n",
    "elif value == 'N':\n",
    "    print(\"Enter your own values:\")\n",
    "    min_d = int(input(\"edim:\"))\n",
    "    nhop = int(input(\"nhop:\"))\n",
    "    mem_size = int(input(\"mem_size:\"))\n",
    "    batch_size = int(input(\"batch_size:\"))\n",
    "    nepoch = int(input(\"nepoch:\"))\n",
    "    anneal_epoch = int(input(\"annel_epoch:\"))\n",
    "    init_lr = float(input(\"init_lr:\"))\n",
    "    anneal_rate = float(input(\"annel_rate:\"))\n",
    "    init_mean = float(input(\"init_mean:\"))\n",
    "    init_std = float(input(\"init_std:\"))\n",
    "    max_grad_norm = int(input(\"max_grand_norm:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = {}\n",
    "test_questions = {}\n",
    "valid_questions = {}\n",
    "# It will create checkpoint directory if not exists\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "keys =[]\n",
    "predicates_file =  open(\"./contextEncoder/Encoded_Data/Predicates_Vectors.txt\")\n",
    "predicates = predicates_file.readlines()\n",
    "predicates= eval(predicates[0].replace(\" ' \",\"\"))   \n",
    "    \n",
    "# Train file\n",
    "train_questions_file= open(\"./contextEncoder/Encoded_Data/Train_Question_answer_FixedLengthVectors.txt\")\n",
    "train_questions_f = train_questions_file.readlines()\n",
    "for i in range(len(train_questions_f)):\n",
    "    train_questions.update(eval(train_questions_f[i].replace(\" ' \",\"\")))\n",
    "    \n",
    "# test file\n",
    "test_questions_file= open(\"./contextEncoder/Encoded_Data/Test_Question_answer_FixedLengthVectors.txt\")\n",
    "test_questions_f = test_questions_file.readlines()\n",
    "for i in range(len(test_questions_f)):\n",
    "    test_questions.update(eval(test_questions_f[i].replace(\" ' \",\"\")))\n",
    "        \n",
    "#Valid file\n",
    "valid_questions_file= open(\"./contextEncoder/Encoded_Data/valid_Question_answer_FixedLengthVectors.txt\")\n",
    "valid_questions_f = valid_questions_file.readlines()\n",
    "for i in range(len(valid_questions_f)):\n",
    "    valid_questions.update(eval(valid_questions_f[i].replace(\" ' \",\"\")))\n",
    "    \n",
    "\n",
    "#predicate_dict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "word_list = []\n",
    "data = [] \n",
    "text_predicates = []\n",
    "input_file = \"../data/ConvAI2/Final_files/train_both_original_final.txt\"\n",
    "with open(input_file, \"r\") as f:\n",
    "        word_list = f.read().split(\"\\n\")\n",
    "for i in range(len(word_list)):\n",
    "    data.append(word_list[i].split(\"\\t\"))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data[i][0].isnumeric():\n",
    "        k =2\n",
    "    else:\n",
    "        k=1\n",
    "    for j in range(k,len(data[i])):\n",
    "        text_predicates.append(data[i][j].strip('][').split(', ') [1])\n",
    "text_predicates = np.array(text_predicates)\n",
    "text_predicates = np.unique(text_predicates)\n",
    "text_predicates= text_predicates.tolist()[1:]\n",
    "for i in range(len(text_predicates)):\n",
    "    text_predicates[i] = [text_predicates[i].strip(\" ' \")]\n",
    "\n",
    "    \n",
    "final_predicate_dict={}\n",
    "predicate_dict = {}\n",
    "vectors_list=[]\n",
    "word_vectors = KeyedVectors.load(\"./contextEncoder/Models/predicate_vectors.kv\", mmap='r')\n",
    "f = open(\"./contextEncoder/Encoded_Data/Predicates_Vectors.txt\", \"a\")\n",
    "f.close()\n",
    "for i in range(len(text_predicates)):\n",
    "    for j in range(len(text_predicates[i])):\n",
    "        vectors_list.extend([[(i+1)*10 for i in word_vectors[text_predicates[i][j]].tolist()]])\n",
    "for i in range(len(vectors_list)):\n",
    "    predicate_dict[i] = vectors_list[i]\n",
    "final_predicate_dict = dict([(tuple(value), key) for key, value in predicate_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "build memory....\n",
      "WARNING:tensorflow:From C:\\Users\\mrper\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "{'loss': 717.8429782089769, 'epoch': 0, 'learning_rate': 0.01, 'validation_loss': 711.8424852627841}\n",
      "{'loss': 663.4260724060124, 'epoch': 1, 'learning_rate': 0.01, 'validation_loss': 701.1198433948864}\n",
      "{'loss': 626.2632094671861, 'epoch': 2, 'learning_rate': 0.01, 'validation_loss': 647.1812940340909}\n",
      "{'loss': 659.739144296792, 'epoch': 3, 'learning_rate': 0.01, 'validation_loss': 663.3694680397728}\n",
      "{'loss': 647.6696764766385, 'epoch': 4, 'learning_rate': 0.01, 'validation_loss': 690.9142091619318}\n",
      "{'loss': 653.3548195184092, 'epoch': 5, 'learning_rate': 0.01, 'validation_loss': 612.9052975852272}\n",
      "{'loss': 618.3538573699684, 'epoch': 6, 'learning_rate': 0.01, 'validation_loss': 628.3712159978693}\n",
      "{'loss': 675.9219154871361, 'epoch': 7, 'learning_rate': 0.01, 'validation_loss': 711.5664186789772}\n",
      "{'loss': 679.9932347652576, 'epoch': 8, 'learning_rate': 0.01, 'validation_loss': 735.2853343394886}\n",
      "{'loss': 715.1587856765935, 'epoch': 9, 'learning_rate': 0.01, 'validation_loss': 737.4474962713068}\n",
      "{'loss': 632.3834994736673, 'epoch': 10, 'learning_rate': 0.01, 'validation_loss': 664.7311244673296}\n",
      "{'loss': 660.2752167778692, 'epoch': 11, 'learning_rate': 0.01, 'validation_loss': 679.3764778053977}\n",
      "{'loss': 681.8235410496437, 'epoch': 12, 'learning_rate': 0.01, 'validation_loss': 718.8228116122159}\n",
      "{'loss': 709.7260081576388, 'epoch': 13, 'learning_rate': 0.01, 'validation_loss': 733.386072265625}\n",
      "{'loss': 680.3216507116473, 'epoch': 14, 'learning_rate': 0.01, 'validation_loss': 762.29594921875}\n",
      "{'loss': 698.7937473088432, 'epoch': 15, 'learning_rate': 0.01, 'validation_loss': 718.583078568892}\n",
      "{'loss': 697.5495963125084, 'epoch': 16, 'learning_rate': 0.01, 'validation_loss': 714.4451161221591}\n",
      "{'loss': 686.8620182230444, 'epoch': 17, 'learning_rate': 0.01, 'validation_loss': 715.6979169034091}\n",
      "{'loss': 694.5192237434135, 'epoch': 18, 'learning_rate': 0.01, 'validation_loss': 723.0542448508522}\n",
      "{'loss': 699.7644444046938, 'epoch': 19, 'learning_rate': 0.01, 'validation_loss': 726.2829747869318}\n",
      "{'loss': 693.8726628860572, 'epoch': 20, 'learning_rate': 0.01, 'validation_loss': 715.2437565696023}\n",
      "{'loss': 695.2516373133599, 'epoch': 21, 'learning_rate': 0.01, 'validation_loss': 739.1932136008522}\n",
      "{'loss': 711.8353510354485, 'epoch': 22, 'learning_rate': 0.01, 'validation_loss': 722.4439037642045}\n",
      "{'loss': 690.4263822483952, 'epoch': 23, 'learning_rate': 0.01, 'validation_loss': 711.5131656605114}\n",
      "{'loss': 695.2354092786294, 'epoch': 24, 'learning_rate': 0.01, 'validation_loss': 712.7819989346591}\n",
      "{'loss': 672.317550251164, 'epoch': 25, 'learning_rate': 0.01, 'validation_loss': 716.8055653409091}\n",
      "{'loss': 200.4401340196076, 'epoch': 26, 'learning_rate': 0.005, 'validation_loss': 135.30331116832386}\n",
      "{'loss': 124.24870428686901, 'epoch': 27, 'learning_rate': 0.005, 'validation_loss': 118.46316372958097}\n",
      "{'loss': 124.91496514552689, 'epoch': 28, 'learning_rate': 0.005, 'validation_loss': 132.79409996448862}\n",
      "{'loss': 129.2776990605547, 'epoch': 29, 'learning_rate': 0.005, 'validation_loss': 132.073316628196}\n",
      "{'loss': 127.74128235896421, 'epoch': 30, 'learning_rate': 0.005, 'validation_loss': 131.12449394087358}\n",
      "{'loss': 132.64618066158695, 'epoch': 31, 'learning_rate': 0.005, 'validation_loss': 151.82508678089488}\n",
      "{'loss': 131.17919100353475, 'epoch': 32, 'learning_rate': 0.005, 'validation_loss': 131.28285986328126}\n",
      "{'loss': 128.03981716185015, 'epoch': 33, 'learning_rate': 0.005, 'validation_loss': 133.552013671875}\n",
      "{'loss': 137.51332132587623, 'epoch': 34, 'learning_rate': 0.005, 'validation_loss': 134.20338924893466}\n",
      "{'loss': 132.94680621179674, 'epoch': 35, 'learning_rate': 0.005, 'validation_loss': 135.51993539151277}\n",
      "{'loss': 135.64889461483958, 'epoch': 36, 'learning_rate': 0.005, 'validation_loss': 139.5288231977983}\n",
      "{'loss': 136.29564391555806, 'epoch': 37, 'learning_rate': 0.005, 'validation_loss': 132.74361259321734}\n",
      "{'loss': 138.7186380165344, 'epoch': 38, 'learning_rate': 0.005, 'validation_loss': 142.9057889737216}\n",
      "{'loss': 139.8592741115669, 'epoch': 39, 'learning_rate': 0.005, 'validation_loss': 152.79367915482953}\n",
      "{'loss': 138.7121691035078, 'epoch': 40, 'learning_rate': 0.005, 'validation_loss': 142.03680126953125}\n",
      "{'loss': 139.6448645601484, 'epoch': 41, 'learning_rate': 0.005, 'validation_loss': 155.84374809126422}\n",
      "{'loss': 131.4052346897336, 'epoch': 42, 'learning_rate': 0.005, 'validation_loss': 140.708470814098}\n",
      "{'loss': 138.55987900694876, 'epoch': 43, 'learning_rate': 0.005, 'validation_loss': 160.7885516246449}\n",
      "{'loss': 133.77492047112653, 'epoch': 44, 'learning_rate': 0.005, 'validation_loss': 141.10573384232956}\n",
      "{'loss': 136.80399154749182, 'epoch': 45, 'learning_rate': 0.005, 'validation_loss': 140.01873603959518}\n",
      "{'loss': 134.9128809314453, 'epoch': 46, 'learning_rate': 0.005, 'validation_loss': 147.61992320667613}\n",
      "{'loss': 142.11757647996055, 'epoch': 47, 'learning_rate': 0.005, 'validation_loss': 145.51811891867897}\n",
      "{'loss': 136.43513648107773, 'epoch': 48, 'learning_rate': 0.005, 'validation_loss': 137.07787577681108}\n",
      "{'loss': 140.62282734009258, 'epoch': 49, 'learning_rate': 0.005, 'validation_loss': 137.127488170277}\n",
      "{'loss': 137.74909306718885, 'epoch': 50, 'learning_rate': 0.005, 'validation_loss': 143.27582086736507}\n",
      "WARNING:tensorflow:From C:\\Users\\mrper\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "{'loss': 22.786134232912463, 'epoch': 51, 'learning_rate': 0.0025, 'validation_loss': 5.16073312516646}\n",
      "{'loss': 5.257433625602675, 'epoch': 52, 'learning_rate': 0.0025, 'validation_loss': 5.123515317049893}\n",
      "{'loss': 5.629541996484766, 'epoch': 53, 'learning_rate': 0.0025, 'validation_loss': 5.032148526278409}\n",
      "{'loss': 5.358712052156475, 'epoch': 54, 'learning_rate': 0.0025, 'validation_loss': 5.391651023171165}\n",
      "{'loss': 5.214581230419988, 'epoch': 55, 'learning_rate': 0.0025, 'validation_loss': 5.58689591702548}\n",
      "{'loss': 5.3048865066953805, 'epoch': 56, 'learning_rate': 0.0025, 'validation_loss': 5.557454008622603}\n",
      "{'loss': 5.394211649553531, 'epoch': 57, 'learning_rate': 0.0025, 'validation_loss': 5.475813146417791}\n",
      "{'loss': 5.421516782177451, 'epoch': 58, 'learning_rate': 0.0025, 'validation_loss': 5.316628233476119}\n",
      "{'loss': 5.358352723676504, 'epoch': 59, 'learning_rate': 0.0025, 'validation_loss': 5.755669195001776}\n",
      "{'loss': 5.1942857768356445, 'epoch': 60, 'learning_rate': 0.0025, 'validation_loss': 5.309499597722834}\n",
      "{'loss': 5.250394061440507, 'epoch': 61, 'learning_rate': 0.0025, 'validation_loss': 5.390983184814453}\n",
      "{'loss': 5.365250061768836, 'epoch': 62, 'learning_rate': 0.0025, 'validation_loss': 5.418245348843661}\n",
      "{'loss': 5.327513026356726, 'epoch': 63, 'learning_rate': 0.0025, 'validation_loss': 5.450564152110707}\n",
      "{'loss': 5.206423597486326, 'epoch': 64, 'learning_rate': 0.0025, 'validation_loss': 5.369579329057173}\n",
      "{'loss': 5.555788783354872, 'epoch': 65, 'learning_rate': 0.0025, 'validation_loss': 5.449759998668324}\n",
      "{'loss': 5.422341692758626, 'epoch': 66, 'learning_rate': 0.0025, 'validation_loss': 5.502891143798828}\n",
      "{'loss': 5.32624500900719, 'epoch': 67, 'learning_rate': 0.0025, 'validation_loss': 5.48354037475586}\n",
      "{'loss': 5.3180782113171565, 'epoch': 68, 'learning_rate': 0.0025, 'validation_loss': 5.845004044966264}\n",
      "{'loss': 5.487454229933973, 'epoch': 69, 'learning_rate': 0.0025, 'validation_loss': 5.550033247514205}\n",
      "{'loss': 5.594360710915779, 'epoch': 70, 'learning_rate': 0.0025, 'validation_loss': 5.598312531904741}\n",
      "{'loss': 5.407712694648944, 'epoch': 71, 'learning_rate': 0.0025, 'validation_loss': 6.102525428078391}\n",
      "{'loss': 5.598581858731728, 'epoch': 72, 'learning_rate': 0.0025, 'validation_loss': 5.7826845009543675}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.134160235756913, 'epoch': 73, 'learning_rate': 0.0025, 'validation_loss': 5.557725140658292}\n",
      "{'loss': 5.353290128009923, 'epoch': 74, 'learning_rate': 0.0025, 'validation_loss': 5.366760025024414}\n",
      "{'loss': 5.403919580727706, 'epoch': 75, 'learning_rate': 0.0025, 'validation_loss': 5.899872068925338}\n",
      "{'loss': 3.8622100735560725, 'epoch': 76, 'learning_rate': 0.00125, 'validation_loss': 3.888650215842507}\n",
      "{'loss': 3.863350242710199, 'epoch': 77, 'learning_rate': 0.00125, 'validation_loss': 3.889740386962891}\n",
      "{'loss': 3.867878820219827, 'epoch': 78, 'learning_rate': 0.00125, 'validation_loss': 3.8924347284490413}\n",
      "{'loss': 3.8650537828206333, 'epoch': 79, 'learning_rate': 0.00125, 'validation_loss': 3.888090178056197}\n",
      "{'loss': 3.865059750830126, 'epoch': 80, 'learning_rate': 0.00125, 'validation_loss': 3.8847736982865766}\n",
      "{'loss': 3.8663758897013225, 'epoch': 81, 'learning_rate': 0.00125, 'validation_loss': 3.885346230246804}\n",
      "{'loss': 3.8646131214950077, 'epoch': 82, 'learning_rate': 0.00125, 'validation_loss': 3.8847762256969105}\n",
      "{'loss': 3.8642693738807146, 'epoch': 83, 'learning_rate': 0.00125, 'validation_loss': 3.885511947631836}\n",
      "{'loss': 3.8660335545450564, 'epoch': 84, 'learning_rate': 0.00125, 'validation_loss': 3.88820024247603}\n",
      "{'loss': 3.8652489871403235, 'epoch': 85, 'learning_rate': 0.00125, 'validation_loss': 3.891160231156783}\n",
      "{'loss': 3.864961142824232, 'epoch': 86, 'learning_rate': 0.00125, 'validation_loss': 3.8905836181640625}\n",
      "{'loss': 3.864560873025533, 'epoch': 87, 'learning_rate': 0.00125, 'validation_loss': 3.8879006361527875}\n",
      "{'loss': 3.8646492979514626, 'epoch': 88, 'learning_rate': 0.00125, 'validation_loss': 3.896370279485529}\n",
      "{'loss': 3.864824981776796, 'epoch': 89, 'learning_rate': 0.00125, 'validation_loss': 3.8922374392422765}\n",
      "{'loss': 3.865207928532678, 'epoch': 90, 'learning_rate': 0.00125, 'validation_loss': 3.8895423722700637}\n",
      "{'loss': 3.8653518613746214, 'epoch': 91, 'learning_rate': 0.00125, 'validation_loss': 3.889016733342951}\n",
      "{'loss': 3.864572610963501, 'epoch': 92, 'learning_rate': 0.00125, 'validation_loss': 3.890198017467152}\n",
      "{'loss': 3.8666375392845502, 'epoch': 93, 'learning_rate': 0.00125, 'validation_loss': 3.8869766263094814}\n",
      "{'loss': 3.865220955400984, 'epoch': 94, 'learning_rate': 0.00125, 'validation_loss': 3.8892137645374647}\n",
      "{'loss': 3.864975085700267, 'epoch': 95, 'learning_rate': 0.00125, 'validation_loss': 3.888865590875799}\n",
      "{'loss': 3.8654155675052224, 'epoch': 96, 'learning_rate': 0.00125, 'validation_loss': 3.8877710820978337}\n",
      "{'loss': 3.8655363914670047, 'epoch': 97, 'learning_rate': 0.00125, 'validation_loss': 3.890672696200284}\n",
      "{'loss': 3.865177053004998, 'epoch': 98, 'learning_rate': 0.00125, 'validation_loss': 3.888447998046875}\n",
      "{'loss': 3.8650452285281225, 'epoch': 99, 'learning_rate': 0.00125, 'validation_loss': 3.8907367095947265}\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    #give length of dictionary of unique word to the model\n",
    "    model = MemN2N(edim, nhop, mem_size, batch_size, nepoch, anneal_epoch, init_lr, anneal_rate, init_mean,\n",
    "                   init_std, max_grad_norm, data_dir, checkpoint_dir, lin_start, is_test, show_progress, sess)\n",
    "        \n",
    "    #calling build_model in model.py\n",
    "    model.build_model()\n",
    "        \n",
    "    if is_test:\n",
    "        model.run(final_predicate_dict,predicates, valid_questions, test_questions)\n",
    "        #model.run(predicates, valid_questions, test_questions)\n",
    "    else:\n",
    "        model.run(final_predicate_dict,predicates, train_questions, valid_questions)\n",
    "        #model.run(predicates, train_questions, valid_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "build memory....\n",
      "WARNING:tensorflow:From C:\\Users\\mrper\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints\\MemN2N.model-50504\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    #give length of dictionary of unique word to the model\n",
    "    model = MemN2N(edim, nhop, mem_size, batch_size, nepoch, anneal_epoch, init_lr, anneal_rate, init_mean,\n",
    "                   init_std, max_grad_norm, data_dir, checkpoint_dir, lin_start, is_test, show_progress, sess)\n",
    "        \n",
    "    #calling build_model in model.py\n",
    "    model.build_model()\n",
    "    predictions, target = model.predict(final_predicate_dict,predicates, train_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual====> has_profession\n",
      "Prediction====> has_profession\n"
     ]
    }
   ],
   "source": [
    "key_list = list(final_predicate_dict.keys()) \n",
    "val_list = list(final_predicate_dict.values())\n",
    "  \n",
    "index = 90 #190\n",
    "question = train_questions[index]['sentance']\n",
    "answer = train_questions[index]['predicate']\n",
    "f_answer = [(i/10)-1 for i in answer]\n",
    "#print(\"question==>\", question)\n",
    "#print(\"answer==>\",f_answer)\n",
    "    \n",
    "#actual\n",
    "actual = word_vectors.most_similar(positive=[np.array(f_answer)], topn=1)\n",
    "print(\"Actual====>\",actual[0][0])\n",
    "    \n",
    "# prediction\n",
    "predicted_predicate_vector= key_list[val_list.index(np.argmax(predictions[index]))] \n",
    "f_predicted_predicate_vector = [(i/10)-1 for i in predicted_predicate_vector]\n",
    "prediction = word_vectors.most_similar(positive=[np.array(f_predicted_predicate_vector)], topn=1)\n",
    "predicate = prediction[0][0]\n",
    "print(\"Prediction====>\",predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Liaison Algorithm for subject and object extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download stanford coreNLP from here:https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "**run this command to start server:** java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pandas as pd, numpy as np\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from nltk.tree import ParentedTree\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000/')\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000/', tagtype='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence (input_sent):\n",
    "    # Parse sentence using Stanford CoreNLP Parser\n",
    "    parse_tree, = ParentedTree.convert(list(pos_tagger.parse(input_sent.split()))[0])\n",
    "    return parse_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject (parse_tree):\n",
    "    # Extract the nouns and adjectives from NP_subtree which is before the first / main VP_subtree\n",
    "    subject, adjective = [],[]\n",
    "    for s in parse_tree:\n",
    "        if s.label() == 'NP':\n",
    "            for t in s.subtrees(lambda y: y.label() in ['NN','NNP','NNS','NNPS','PRP']):\n",
    "                # Avoid empty or repeated values\n",
    "                if t.pos()[0] not in subject:\n",
    "                    subject.append(t.pos()[0])\n",
    "            for t in s.subtrees(lambda y: y.label().startswith('JJ')):\n",
    "                if t.pos()[0] not in adjective:\n",
    "                    adjective.append(t.pos()[0])\n",
    "    return subject, adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object (parse_tree):\n",
    "    # Extract the nouns from VP_NP_subtree\n",
    "    objects, output = [],[]\n",
    "    for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
    "        for t in s.subtrees(lambda y: y.label() == 'NP'):\n",
    "            for u in t.subtrees(lambda z: z.label() in ['NN','NNP','NNS','NNPS','PRP$']):\n",
    "                output = u.pos()[0]\n",
    "                if u.left_sibling() is not None and u.left_sibling().label().startswith('JJ'):\n",
    "                    output += u.left_sibling().pos()[0]\n",
    "                if output not in objects:\n",
    "                    objects.append(output)\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_liaison (input_sent, output=['tagging','parse_tree','type_dep','spo']):\n",
    "    parse_tree= convert_sentence(input_sent)\n",
    "    sub_obj = []\n",
    "    # Extract subject, predicate and object\n",
    "    subject, adjective = get_subject(parse_tree)\n",
    "    objects = get_object(parse_tree)\n",
    "    if 'parse_tree' in output:\n",
    "        print('---PARSE TREE---')\n",
    "        parse_tree.pretty_print()\n",
    "        print()\n",
    "    if 'spo' in output:\n",
    "        print('---MULTI-LIAISON OUTPUT---')\n",
    "        print('Subject: ')\n",
    "        print(subject[0][0])\n",
    "        print('Object: ')\n",
    "        print(objects[0][0])\n",
    "        print()\n",
    "        sub_obj.append(subject[0][0])\n",
    "        sub_obj.append(objects[0][0])\n",
    "    return sub_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol , i can imagine . i will be reading a lot when football is over'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "word_list = []\n",
    "data = [] \n",
    "input_file = \"../data/ConvAI2/Final_files/train_both_original_final_filter.txt\"\n",
    "with open(input_file, \"r\") as f:\n",
    "        word_list = f.read().split(\"\\n\")\n",
    "#word_list\n",
    "for i in range(len(word_list)):\n",
    "     data.append(word_list[i].split(\"\\t\"))\n",
    "\n",
    "if data[index][0].isnumeric():\n",
    "    sentance = data[index][1]\n",
    "else:\n",
    "    sentance = data[index][0] \n",
    "sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---PARSE TREE---\n",
      "                                      S                                                            \n",
      "  ____________________________________|____                                                         \n",
      " |            |                   |        VP                                                      \n",
      " |            |                   |    ____|_____                                                   \n",
      " |            |                   |   |          VP                                                \n",
      " |            |                   |   |     _____|_____                                             \n",
      " |            |                   |   |    |           VP                                          \n",
      " |            |                   |   |    |      _____|____________                                \n",
      " |            |                   |   |    |     |                  NP                             \n",
      " |            |                   |   |    |     |          ________|______________                 \n",
      " |           PRN                  |   |    |     |         |                      SBAR             \n",
      " |            |                   |   |    |     |         |         ______________|____            \n",
      " |            S                   |   |    |     |         |        |                   S          \n",
      " |     _______|_______________    |   |    |     |         |        |        ___________|___        \n",
      " |    |   |       VP          |   |   |    |     |         |        |       |               VP     \n",
      " |    |   |    ___|_____      |   |   |    |     |         |        |       |            ___|___    \n",
      "INTJ  |   NP  |         VP    |   NP  |    |     |         NP     WHADVP    NP          |      ADJP\n",
      " |    |   |   |         |     |   |   |    |     |      ___|___     |       |           |       |   \n",
      " UH   ,  PRP  MD        VB    .  PRP  MD   VB   VBG    DT      NN  WRB      NN         VBZ      RB \n",
      " |    |   |   |         |     |   |   |    |     |     |       |    |       |           |       |   \n",
      "lol   ,   i  can     imagine  .   i  will  be reading  a      lot  when  football       is     over\n",
      "\n",
      "\n",
      "---MULTI-LIAISON OUTPUT---\n",
      "Subject: \n",
      "i\n",
      "Object: \n",
      "lot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_obj = multi_liaison(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sub_obj = sub_obj[:1] + [predicate] + sub_obj[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 'has_profession', 'lot')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple = tuple(f_sub_obj)\n",
    "triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
